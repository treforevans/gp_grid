{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 14:37:20 ] matplotlib.backends DEBUG: backend module://ipykernel.pylab.backend_inline version unknown\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import gp_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from gp_grid.misc import rastrigin, grid2mat, gapify_data\n",
    "from time import time\n",
    "from pdb import set_trace\n",
    "gp_grid.debug()\n",
    "to_plot = False\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from numpy.testing import assert_array_almost_equal\n",
    "import GPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test `gp_grid.models.GPGappyRegression`\n",
    "## Test the Posterior Variance\n",
    "TODO: also do the posterior mean check here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 14:37:20 ] gp_grid.kern DEBUG: Initializing RBF kernel.\n",
      "[ 14:37:20 ] gp_grid.kern DEBUG: Initializing RBF kernel.\n",
      "[ 14:37:20 ] GP INFO: initializing Y\n",
      "[ 14:37:20 ] GP INFO: initializing inference method\n",
      "[ 14:37:20 ] GP INFO: adding kernel and likelihood as parameters\n",
      "[ 14:37:20 ] gp_grid.models DEBUG: Initializing GPGappyRegression model.\n",
      "done tests.\n"
     ]
    }
   ],
   "source": [
    "# get the data\n",
    "np.random.seed(0)\n",
    "d = 2\n",
    "\n",
    "# generate the full training set\n",
    "n = 20 # number of points along each dimension\n",
    "xg = [np.linspace(0,1,num=n).reshape((-1,1)),]*d\n",
    "x = grid2mat(*xg)\n",
    "y_full = rastrigin((x*2-1)*5.12)\n",
    "y_scaler = StandardScaler().fit(y_full)\n",
    "y_full = y_scaler.transform(y_full)\n",
    "yg_full = y_full.reshape((n,)*d)\n",
    "N = n**d\n",
    "\n",
    "# hyperparameters to use (from nips_gappy/studies/gappiness sweep)\n",
    "kern_list = [gp_grid.kern.RBF(1, lengthscale=0.0652723) for i in range(d)]\n",
    "kern_list[0].variance = 166.298\n",
    "noise_var = 1e-5\n",
    "\n",
    "# randomly apply the mask to the gaps\n",
    "gappiness = 0.25\n",
    "gaps,y = gapify_data(y_full,gappiness)\n",
    "yg = y.reshape((n,)*d)\n",
    "\n",
    "# get the input position of the gaps and the training data\n",
    "n_gaps = gaps.size\n",
    "n_train = y.size-n_gaps\n",
    "X = np.delete(x, gaps, axis=0) # start with all the points then remove gaps\n",
    "Y = np.delete(y, gaps, axis=0) # ... same here\n",
    "Xt = np.zeros((n_gaps,d))\n",
    "idx = np.unravel_index(gaps,(n,)*d)\n",
    "for i_d in range(d):\n",
    "    Xt[:,i_d] = xg[i_d][idx[i_d],0]\n",
    "\n",
    "# train a gpy model\n",
    "mgpy = GPy.models.GPRegression(X,Y,GPy.kern.RBF(2,variance=166.298,lengthscale=0.0652723), noise_var=noise_var)\n",
    "gpy_cov = mgpy.predict(Xt,full_cov=True)[1]\n",
    "\n",
    "# train gp_grid model\n",
    "m = gp_grid.models.GPGappyRegression(xg,yg,kern_list,noise_var=noise_var)\n",
    "for key, value in {'iterative_formulation':'IG','preconditioner':'rank-reduced','n_eigs':300}.iteritems():\n",
    "    setattr(m,key,value)\n",
    "m.pcg_options['maxiter'] = 2000\n",
    "m.pcg_options['tol'] = 1e-6\n",
    "m.dim_noise_var = 0.\n",
    "m.fit()\n",
    "\n",
    "# compute the posterior variance\n",
    "my_cov = m.predict_cov(X_new=Xt, exact=True)\n",
    "\n",
    "# check if equal\n",
    "assert_array_almost_equal(gpy_cov,my_cov,decimal=3)\n",
    "print \"done tests.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Preconditioners\n",
    "\n",
    "First get the dataset with a random gappy response vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 14:38:17 ] gp_grid.kern DEBUG: Initializing RBF kernel.\n",
      "[ 14:38:17 ] gp_grid.kern DEBUG: Initializing RBF kernel.\n",
      "[ 14:38:17 ] gp_grid.models DEBUG: Initializing GPGappyRegression model.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "d = 2\n",
    "n = 40\n",
    "N = n**d\n",
    "p = 400 # num eigvals\n",
    "lengthscale = 0.05\n",
    "xg = [np.linspace(0,1,num=n).reshape((-1,1)),]*d # d-dimensional grid\n",
    "yg = gapify_data(np.random.normal(loc=0.0, scale=1.0, size=(N,1)), gappiness=0.5)[1].reshape((n,)*d)\n",
    "y = yg.reshape((-1,1))\n",
    "nominal = np.linalg.norm(y) # nominal residual (with alp = 0)\n",
    "kern_list = [gp_grid.kern.RBF(1, lengthscale=lengthscale) for i in range(d)] #initialize the kernel\n",
    "noise_var = 1e-6\n",
    "m = gp_grid.models.GPGappyRegression(xg,yg,kern_list,noise_var=noise_var, \n",
    "                                     iterative_formulation=\"IG\") # initialize the model\n",
    "m.n_eigs = p\n",
    "m.pcg_options['maxiter']     = 200000\n",
    "m.pcg_options['tol']         = 1e-6\n",
    "m.parameters; # set internal state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for both the FG and IG methods, check that adding a preconditioner decreases the condition number of the system of equations which needs to be solved.\n",
    "\n",
    "Additionally, check that the method converges faster with the use of the preconditioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IG nominal cond: 4528191.40, modified cond: 455002.49\n",
      "IG factional condition number: 0.100\n",
      "    time w/ None precon = 1.41 seconds\n",
      "    time w/ 'rank-reduced' precon = 0.70 seconds\n",
      "FG nominal cond: 332954.73, modified cond: 97532.62\n",
      "FG factional condition number: 0.293\n",
      "    time w/ None precon = 1.41 seconds\n",
      "    time w/ 'rank-reduced' precon = 0.69 seconds\n"
     ]
    }
   ],
   "source": [
    "assert m.iterative_formulation == \"IG\" # must start with this\n",
    "# set up the preconditioners\n",
    "m._fg_preconditioner()\n",
    "m._rank_reduced_cov() # setup the rank reduced approx\n",
    "\n",
    "for method in ['IG', 'FG']:\n",
    "    # get matrix vector routines\n",
    "    if method == 'IG':\n",
    "        size = m.n_not_gaps\n",
    "        mv = m._mv_IG\n",
    "        precon_mv = m._mv_rr_cov_inv\n",
    "    elif method == 'FG':\n",
    "        size = m.n_gaps\n",
    "        mv = m._mv_FG\n",
    "        precon_mv = m._mv_rr_fg_precon\n",
    "        \n",
    "    # initialize matricies and compute\n",
    "    nominal_mat = np.zeros((size,size))\n",
    "    modified_mat = np.zeros((size,size))\n",
    "    for i, e_i in enumerate(np.identity(size)):\n",
    "        nominal_mat[:,i,None] = mv(e_i.reshape((-1,1)))\n",
    "        modified_mat[:,i,None] = precon_mv(nominal_mat[:,i,None])\n",
    "    nom_cond = np.linalg.cond(nominal_mat)\n",
    "    mod_cond = np.linalg.cond(modified_mat)\n",
    "    print \"%s nominal cond: %.2f, modified cond: %.2f\" % (method,nom_cond,mod_cond)\n",
    "    print \"%s factional condition number: %.3f\" % (method,mod_cond/nom_cond)\n",
    "    assert mod_cond/nom_cond < 1, \"condition number didn't decrease\"\n",
    "    \n",
    "    # check the time required to fit with and without precon\n",
    "    times = np.zeros(2)\n",
    "    for i,precon in enumerate([None, 'rank-reduced']):\n",
    "        m._alpha = None\n",
    "        m.preconditioner = precon\n",
    "        t0 = time()\n",
    "        m.fit()\n",
    "        t1 = time()\n",
    "        print '    time w/ %s precon = %.2f seconds' % (repr(precon),t1-t0)\n",
    "        times[i] = t1-t0\n",
    "    assert times[1] < times[0], \"preconditioner didn't expedite convergence\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Training the Models Through the different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "d = 2\n",
    "\n",
    "# generate the training set\n",
    "n = 25 # number of points along each dimension\n",
    "N_missing = np.int32(np.floor(0.25*n**d))\n",
    "xg = [np.linspace(0,1,num=n).reshape((-1,1)),]*d\n",
    "x = grid2mat(*xg)\n",
    "y = rastrigin((x*2-1)*5.12)\n",
    "gaps = np.random.choice(y.shape[0], size=N_missing, replace=False) # randomly set some data to nan\n",
    "y[gaps] = np.nan\n",
    "yg = y.reshape((n,)*d)\n",
    "# plt.imshow(yg,interpolation='none') # plot the training data\n",
    "\n",
    "# generate the test set\n",
    "nn = 100 # number of test points along each dimension\n",
    "xxg = [np.linspace(0,1,num=nn).reshape((-1,1)),]*d\n",
    "xx = grid2mat(*xxg) \n",
    "yy = rastrigin((xx*2-1)*5.12)\n",
    "\n",
    "def plot_model(m):\n",
    "    plt.figure()\n",
    "    yyh = m.predict_grid(xxg) # the model prediction\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(yg,interpolation='none')\n",
    "    plt.title('training data')\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(yyh.reshape((nn,)*d),interpolation='none')\n",
    "    plt.title('prediction')\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(yy.reshape((nn,)*d),interpolation='none')\n",
    "    plt.title('exact')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill Gaps MLE with preconditioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 14:41:42 ] gp_grid.kern DEBUG: Initializing RBF kernel.\n",
      "[ 14:41:42 ] gp_grid.kern DEBUG: Initializing RBF kernel.\n",
      "[ 14:41:42 ] gp_grid.models DEBUG: Initializing GPGappyRegression model.\n",
      "[ 14:41:48 ] gp_grid.models INFO: pcg convergence to tolerance not achieved. Number of iterations: 10000\n",
      "[ 14:41:53 ] gp_grid.models INFO: pcg convergence to tolerance not achieved. Number of iterations: 10000\n",
      "[ 14:41:59 ] gp_grid.models INFO: pcg convergence to tolerance not achieved. Number of iterations: 10000\n",
      "[ 14:42:05 ] gp_grid.models INFO: pcg convergence to tolerance not achieved. Number of iterations: 10000\n",
      "[ 14:42:11 ] gp_grid.models INFO: pcg convergence to tolerance not achieved. Number of iterations: 10000\n",
      "[ 14:42:16 ] gp_grid.models INFO: pcg convergence to tolerance not achieved. Number of iterations: 10000\n",
      "[ 14:42:16 ] gp_grid.models INFO: Gradient check passed.\n",
      "[ 14:42:16 ] gp_grid.models DEBUG: Beginning MLE to optimize hyperparameters. grad_method=adjoint\n",
      "[ 14:42:16 ] gp_grid.models INFO: Function Evals: 2. Log-Marginal Likelihood: -2238.01. Exit status: 1\n",
      "[ 14:42:17 ] gp_grid.models DEBUG: using the exact log_det for log likelihood. May be expensive!\n",
      "exact log-likelihood:    -2259.84\n",
      "computed log-likelihood: -2238.01\n"
     ]
    }
   ],
   "source": [
    "kern_list = [gp_grid.kern.RBF(1, lengthscale=0.05) for i in range(d)]\n",
    "kern_list[0].variance = 875.\n",
    "m = gp_grid.models.GPGappyRegression(xg,yg,kern_list,noise_var=1.1e-6, \n",
    "                                     iterative_formulation = 'FG', preconditioner_rank=10)\n",
    "m.MLE_method = 'iterative'\n",
    "m.grad_method = 'adjoint'\n",
    "m.checkgrad()\n",
    "m.optimize()\n",
    "m.fit()\n",
    "exact_ll = m._compute_log_likelihood(parameters=m.parameters,exact_det=True) # this is the exact log_likelihood\n",
    "approx_ll = m._compute_log_likelihood(parameters=m.parameters,exact_det=False)\n",
    "print 'exact log-likelihood:    %g' % exact_ll\n",
    "print 'computed log-likelihood: %g' % approx_ll\n",
    "assert np.abs((exact_ll-approx_ll)/exact_ll) < 0.05, \"log-likelihood relative error too large\"\n",
    "if to_plot:\n",
    "    plot_model(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill Gaps MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 14:42:17 ] gp_grid.kern DEBUG: Initializing RBF kernel.\n",
      "[ 14:42:17 ] gp_grid.kern DEBUG: Initializing RBF kernel.\n",
      "[ 14:42:17 ] gp_grid.models DEBUG: Initializing GPGappyRegression model.\n",
      "[ 14:42:20 ] gp_grid.models INFO: pcg convergence to tolerance not achieved. Number of iterations: 10000\n",
      "[ 14:42:22 ] gp_grid.models INFO: pcg convergence to tolerance not achieved. Number of iterations: 10000\n",
      "[ 14:42:25 ] gp_grid.models INFO: pcg convergence to tolerance not achieved. Number of iterations: 10000\n",
      "[ 14:42:27 ] gp_grid.models INFO: pcg convergence to tolerance not achieved. Number of iterations: 10000\n",
      "[ 14:42:30 ] gp_grid.models INFO: pcg convergence to tolerance not achieved. Number of iterations: 10000\n",
      "[ 14:42:33 ] gp_grid.models INFO: pcg convergence to tolerance not achieved. Number of iterations: 10000\n",
      "[ 14:42:33 ] gp_grid.models INFO: Gradient check passed.\n",
      "[ 14:42:33 ] gp_grid.models DEBUG: Beginning MLE to optimize hyperparameters. grad_method=adjoint\n",
      "[ 14:42:34 ] gp_grid.models INFO: Function Evals: 15. Log-Marginal Likelihood: -2090.81.\n",
      "[ 14:42:34 ] gp_grid.models DEBUG: using the exact log_det for log likelihood. May be expensive!\n",
      "exact log-likelihood:    -2051.81\n",
      "computed log-likelihood: -2090.81\n"
     ]
    }
   ],
   "source": [
    "kern_list = [gp_grid.kern.RBF(1, lengthscale=0.05) for i in range(d)]\n",
    "kern_list[0].variance = 875.\n",
    "m = gp_grid.models.GPGappyRegression(xg,yg,kern_list,noise_var=1.1e-6)\n",
    "m.iterative_formulation = 'FG'\n",
    "m.MLE_method = 'iterative'\n",
    "m.grad_method = 'adjoint'\n",
    "m.preconditioner = None\n",
    "m.checkgrad()\n",
    "m.optimize()\n",
    "m.fit()\n",
    "exact_ll = m._compute_log_likelihood(parameters=m.parameters,exact_det=True) # this is the exact log_likelihood\n",
    "approx_ll = m._compute_log_likelihood(parameters=m.parameters,exact_det=False)\n",
    "print 'exact log-likelihood:    %g' % exact_ll\n",
    "print 'computed log-likelihood: %g' % approx_ll\n",
    "assert np.abs((exact_ll-approx_ll)/exact_ll) < 0.05, \"log-likelihood relative error too large\"\n",
    "if to_plot:\n",
    "    plot_model(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignore Gaps MLE with Preconditioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 14:42:40 ] gp_grid.kern DEBUG: Initializing RBF kernel.\n",
      "[ 14:42:40 ] gp_grid.kern DEBUG: Initializing RBF kernel.\n",
      "[ 14:42:40 ] gp_grid.models DEBUG: Initializing GPGappyRegression model.\n",
      "[ 14:42:45 ] gp_grid.models INFO: pcg convergence to tolerance not achieved. Number of iterations: 10000\n",
      "[ 14:42:50 ] gp_grid.models INFO: pcg convergence to tolerance not achieved. Number of iterations: 10000\n",
      "[ 14:42:55 ] gp_grid.models INFO: pcg convergence to tolerance not achieved. Number of iterations: 10000\n",
      "[ 14:43:00 ] gp_grid.models INFO: pcg convergence to tolerance not achieved. Number of iterations: 10000\n",
      "[ 14:43:05 ] gp_grid.models INFO: pcg convergence to tolerance not achieved. Number of iterations: 10000\n",
      "[ 14:43:10 ] gp_grid.models INFO: pcg convergence to tolerance not achieved. Number of iterations: 10000\n",
      "[ 14:43:10 ] gp_grid.models INFO: Gradient check passed.\n",
      "[ 14:43:10 ] gp_grid.models DEBUG: Beginning MLE to optimize hyperparameters. grad_method=adjoint\n",
      "[ 14:43:16 ] gp_grid.models INFO: pcg convergence to tolerance not achieved. Number of iterations: 10000\n",
      "[ 14:43:21 ] gp_grid.models INFO: pcg convergence to tolerance not achieved. Number of iterations: 10000\n",
      "[ 14:43:26 ] gp_grid.models INFO: Function Evals: 19. Log-Marginal Likelihood: -2090.82.\n",
      "[ 14:43:27 ] gp_grid.models DEBUG: using the exact log_det for log likelihood. May be expensive!\n",
      "exact log-likelihood:    -2051.52\n",
      "computed log-likelihood: -2090.82\n"
     ]
    }
   ],
   "source": [
    "kern_list = [gp_grid.kern.RBF(1, lengthscale=0.05) for i in range(d)]\n",
    "kern_list[0].variance = 875.\n",
    "m = gp_grid.models.GPGappyRegression(xg,yg,kern_list,noise_var=1.1e-6)\n",
    "m.iterative_formulation = 'IG'\n",
    "m.MLE_method = 'iterative'\n",
    "m.grad_method = 'adjoint'\n",
    "m.preconditioner = 'rank-reduced'\n",
    "m.n_eigs = 100\n",
    "m.checkgrad()\n",
    "m.optimize()\n",
    "m.fit()\n",
    "exact_ll = m._compute_log_likelihood(parameters=m.parameters,exact_det=True) # this is the exact log_likelihood\n",
    "approx_ll = m._compute_log_likelihood(parameters=m.parameters,exact_det=False)\n",
    "print 'exact log-likelihood:    %g' % exact_ll\n",
    "print 'computed log-likelihood: %g' % approx_ll\n",
    "assert np.abs((exact_ll-approx_ll)/exact_ll) < 0.05, \"log-likelihood relative error too large\"\n",
    "if to_plot:\n",
    "    plot_model(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank-Reduced MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 14:43:27 ] gp_grid.kern DEBUG: Initializing RBF kernel.\n",
      "[ 14:43:27 ] gp_grid.kern DEBUG: Initializing RBF kernel.\n",
      "[ 14:43:27 ] gp_grid.models DEBUG: Initializing GPGappyRegression model.\n",
      "[ 14:43:27 ] gp_grid.models INFO: Gradient check passed.\n",
      "[ 14:43:27 ] gp_grid.models DEBUG: Beginning MLE to optimize hyperparameters. grad_method=finite_difference\n",
      "[ 14:43:35 ] gp_grid.models INFO: Function Evals: 41. Log-Marginal Likelihood: -1758.59.\n",
      "[ 14:43:35 ] gp_grid.models DEBUG: using the exact log_det for log likelihood. May be expensive!\n",
      "exact log-likelihood:    4762.86\n",
      "computed log-likelihood: -1758.59\n"
     ]
    }
   ],
   "source": [
    "kern_list = [gp_grid.kern.RBF(1, lengthscale=0.05) for i in range(d)]\n",
    "kern_list[0].variance = 875.\n",
    "m = gp_grid.models.GPGappyRegression(xg,yg,kern_list,noise_var=1.1e-6)\n",
    "m.iterative_formulation = 'IG' # this will be used for the final build\n",
    "m.MLE_method = 'rank-reduced'\n",
    "m.grad_method = 'finite_difference'\n",
    "m.preconditioner = None # this has no effect for rank reduced\n",
    "m.n_eigs = 100 # this has a strong dependency on the quality of the final result\n",
    "m.checkgrad()\n",
    "m.optimize()\n",
    "m.fit()\n",
    "exact_ll = m._compute_log_likelihood(parameters=m.parameters,exact_det=True) # this is the exact log_likelihood\n",
    "approx_ll = m._compute_log_likelihood(parameters=m.parameters,exact_det=False)\n",
    "print 'exact log-likelihood:    %g' % exact_ll\n",
    "print 'computed log-likelihood: %g' % approx_ll\n",
    "if to_plot:\n",
    "    plot_model(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalize Gaps MLE Formulation with Preconditioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 14:43:35 ] gp_grid.kern DEBUG: Initializing RBF kernel.\n",
      "[ 14:43:35 ] gp_grid.kern DEBUG: Initializing RBF kernel.\n",
      "[ 14:43:35 ] gp_grid.models DEBUG: Initializing GPGappyRegression model.\n",
      "[ 14:43:36 ] gp_grid.models INFO: pcg convergence to tolerance not achieved. Number of iterations: 10000\n",
      "[ 14:43:37 ] gp_grid.models INFO: pcg convergence to tolerance not achieved. Number of iterations: 10000\n",
      "[ 14:43:38 ] gp_grid.models INFO: pcg convergence to tolerance not achieved. Number of iterations: 10000\n",
      "[ 14:43:39 ] gp_grid.models INFO: pcg convergence to tolerance not achieved. Number of iterations: 10000\n",
      "[ 14:43:41 ] gp_grid.models INFO: pcg convergence to tolerance not achieved. Number of iterations: 10000\n",
      "[ 14:43:42 ] gp_grid.models INFO: pcg convergence to tolerance not achieved. Number of iterations: 10000\n",
      "[ 14:43:42 ] gp_grid.models INFO: Gradient check passed.\n",
      "[ 14:43:42 ] gp_grid.models DEBUG: Beginning MLE to optimize hyperparameters. grad_method=adjoint\n",
      "[ 14:43:43 ] gp_grid.models INFO: pcg convergence to tolerance not achieved. Number of iterations: 10000\n",
      "[ 14:43:44 ] gp_grid.models INFO: pcg convergence to tolerance not achieved. Number of iterations: 10000\n",
      "[ 14:43:46 ] gp_grid.models INFO: Function Evals: 14. Log-Marginal Likelihood: -2102.67.\n",
      "[ 14:43:46 ] gp_grid.models DEBUG: using the exact log_det for log likelihood. May be expensive!\n",
      "exact log-likelihood:    -2063.45\n",
      "computed log-likelihood: -2102.67\n"
     ]
    }
   ],
   "source": [
    "kern_list = [gp_grid.kern.RBF(1, lengthscale=0.05) for i in range(d)]\n",
    "kern_list[0].variance = 875.\n",
    "m = gp_grid.models.GPGappyRegression(xg,yg,kern_list,noise_var=1.1e-6)\n",
    "m.iterative_formulation = 'PG' \n",
    "m.MLE_method = 'iterative'\n",
    "m.grad_method = 'adjoint'\n",
    "m.preconditioner = 'wilson'\n",
    "m.checkgrad()\n",
    "m.optimize()\n",
    "m.fit()\n",
    "exact_ll = m._compute_log_likelihood(parameters=m.parameters,exact_det=True) # this is the exact log_likelihood\n",
    "approx_ll = m._compute_log_likelihood(parameters=m.parameters,exact_det=False)\n",
    "print 'exact log-likelihood:    %g' % exact_ll\n",
    "print 'computed log-likelihood: %g' % approx_ll\n",
    "assert np.abs((exact_ll-approx_ll)/exact_ll) < 0.05, \"log-likelihood relative error too large\"\n",
    "if to_plot:\n",
    "    plot_model(m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27_GPy]",
   "language": "python",
   "name": "conda-env-py27_GPy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
